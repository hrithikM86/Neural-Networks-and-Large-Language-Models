{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2a2626b9-7010-4a5b-bbba-31665cfd9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'C:\\Users\\Mahek\\Downloads\\archive (3)\\sentiment_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d418286f-0fa2-4ca6-946f-5d579de0b6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok brokeback mountain is such a horrible movie.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brokeback Mountain was so awesome.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friday hung out with kelsie and we went and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am going to start reading the Harry Potter s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is it just me, or does Harry Potter suck?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0    Ok brokeback mountain is such a horrible movie.      0\n",
       "1                 Brokeback Mountain was so awesome.      1\n",
       "2  friday hung out with kelsie and we went and sa...      0\n",
       "3  I am going to start reading the Harry Potter s...      1\n",
       "4       Is it just me, or does Harry Potter suck?...      0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "855283b5-67d0-451d-a762-84a5bfef50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['sentence'], df['label'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.3,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.5,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = temp_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1b01d0a3-abb0-4044-aa0c-35831d67d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f41ae337-f2a5-4767-9f0b-bd6425ed1c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1504.,  701.,  572.,  521.,  164.,  215.,  117.,   63.,  104.,\n",
       "           6.]),\n",
       " array([ 3. ,  6.5, 10. , 13.5, 17. , 20.5, 24. , 27.5, 31. , 34.5, 38. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_lens = [len(i.split()) for i in train_text]\n",
    "plt.hist(train_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d99aa9e9-f102-4bb8-a6b4-9abe4cea9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences\n",
    "import torch\n",
    "pad_len = 17\n",
    "# Instead of using pad_to_max_length, use padding='max_length' and max_length argument\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    padding='max_length',  # Use padding='max_length'\n",
    "    max_length=pad_len,     # Specify the maximum length\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    padding='max_length',  # Use padding='max_length'\n",
    "    max_length=pad_len,     # Specify the maximum length\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    padding='max_length',  # Use padding='max_length'\n",
    "    max_length=pad_len,     # Specify the maximum length\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3b0539fd-0f27-476b-9306-f1d87f71b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the pretrained layers\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "  \n",
    "#defining new layers\n",
    "class BERT_architecture(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "      super(BERT_architecture, self).__init__()\n",
    "  \n",
    "      self.bert = bert \n",
    "        \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "  \n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "  \n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, sent_id, mask):\n",
    "  \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        x = self.fc1(cls_hs)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "19ba2692-5749-42c9-bb07-098fc0f46434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BERT_architecture(bert)\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) # learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "39c45b3b-3353-4402-aa9a-c6b6b9b66bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cf5084ed-4f3d-4dd1-b390-c8db2749d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    \n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "      # push the batch to gp\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        \n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        \n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "104a1c2e-f6d4-411e-a653-9bb8ee68d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "  # deactivate dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "      # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "              # # Calculate elapsed time in minutes.\n",
    "      # elapsed = format_time(time.time() - t0)\n",
    "              \n",
    "      # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "        batch = [t.to(device) for t in batch]\n",
    "  \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            \n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            \n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            \n",
    "            total_preds.append(preds)\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    return avg_loss, total_preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e781b81-2959-4530-b52d-da71f765ffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.30      0.35       370\n",
      "           1       0.56      0.69      0.62       481\n",
      "\n",
      "    accuracy                           0.52       851\n",
      "   macro avg       0.49      0.49      0.48       851\n",
      "weighted avg       0.50      0.52      0.50       851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\t\n",
    "from sklearn.metrics import classification_report\n",
    "pred = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2d6a91ef-1d87-4f26-b372-f308dbd34d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eb7ce0e8-221b-4e34-a2fb-2198f2526e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors=\"pt\", max_length=17, pad_to_max_length=True, truncation=True)\n",
    "    sent_id = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(sent_id, mask)\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    return {\"negative\": float(probs[0]), \"positive\": float(probs[1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "abb38213-480b-4dd4-adba-4bf279427b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(fn=sentiment_analysis, inputs=\"text\", outputs=\"label\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23375df-cbab-4edf-b96b-10b82d6c001e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916277c-3ab7-4af7-a325-9de9a4b12f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
